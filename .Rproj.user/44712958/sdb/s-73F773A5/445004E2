{
    "collab_server" : "",
    "contents" : "---\ntitle: \"An Introduction To The `fTree` Package\"\nauthor: \"Ognjen Grujic (ognjengr@gmail.com)\"\ndate: \"November 16th 2017\"\noutput: \n    html_document:\n        theme: default\n        highlight: tango\n        toc: yes\n        toc_float: true\n        toc_depth: 2\n---\n<style type=\"text/css\">\n\nh1.title {\n  font-size: 38px;\n  color: Black;\n  text-align: center;\n}\nh4.author { /* Header 4 - and the author and data headers use this too  */\n    font-size: 18px;\n  font-family: \"Times New Roman\", Times, serif;\n  color: Black;\n  text-align: center;\n}\nh4.date { /* Header 4 - and the author and data headers use this too  */\n  font-size: 18px;\n  font-family: \"Times New Roman\", Times, serif;\n  color: Black;\n  text-align: center;\n}\n\nbody {\n    position: center;\n    text-align: justify;\n    text-justify: inter-word;\n    vertical-align: middle;\n}\n</style>\n\n# Introduction\n\nRegression trees, and their boostrapped derivatives, are ones of the most popular off the shelf machine learning techniques. Their popularity is due to their high interpretability and exceptionaly great predictive power. The original regression tree developments by Breiman et al (1984) considered scalars or categories as output variables. In modern applications this represents a significant limitation given that many modern datasets contain output variables that are multidimensional such as vectors, or even infinite dimensional such as curves or functions. To expand the applicability of regression trees for such complex output data [Segal (1992)](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1992.10475220) and [Segal (2011)](http://onlinelibrary.wiley.com/doi/10.1002/widm.12/full) developed strategies for growing regression trees when the output data are vectors, while [Yu and Lambert (1998)](http://amstat.tandfonline.com/doi/abs/10.1080/10618600.1999.10474847) applied the same strategies in the context of infinite dimensional output variables. While all these expansions are theoretically sound and were shown to work great in their respective publications, freely available computer codes that implement the methods were never released. This significantly limited the accessibility and the adoption of the methods in the scientific and engineering communities. In this document we introduce the `fTree` package that implements all of the aforementioned expansions of regression trees and several other experimental techniques proposed in [Grujic (2017)](https://pangea.stanford.edu/ERE/pdf/pereports/PhD/Grujic2017.pdf?). We start this document with a brief review of regression trees and then we proceed to discuss the expansion strategies for high dimensional output data. At the end of this document we also discuss a strategy for growing regression trees when the input data are functions. The package was developed for both scientific and engineering usage and a special emphasis was given to expendability which we discuss very last. \n\n# Regression Trees\nConsider a training set $\\mathcal{T}:\\{(\\boldsymbol{x_i}, y_i)\\}_{i=1}^{N}$ where $\\boldsymbol{x}_i$ is a vector in $R^n$ and $y_i$ is a scalar output ($y_i \\in R$), and let $f: \\boldsymbol{x} \\rightarrow y$ be a function that maps $\\boldsymbol{x}_i$'s to $y_i$'s.  The idea of regression trees is to partition the $p$-dimensional input space spanned by $\\boldsymbol{x}_i$'s into $M$ disjoint sub-regions $R_m$'s, and then in every sub region approximate the true function $f$ with some local function $f_m$. This local function can be a constant (i.e. the local mean), a local regression or even a Gaussian process. Input space partitioning can be performed in many ways, however, one of the most widely adopted partitioning schemes is the binary recursive splitting method proposed by \\cite{Breiman_Friedman_Stone_Olshen_1984}. Their method is binary because it considers binary splits of $R^n$ along one predictor (covariate) at a time. It is recursive because regions are recursively split into sub-regions until some stopping criteria is met, or, no more training data is left to split. The procedure is also greedy since, in determining the best split of a region it does not consider the quality of later splits. To determine the best split of one region, the method relies on user specified cost function $G$. For example, when considering some split $s$ along predictor $p$ of region $R_m$ into two sub-regions $R_{ml}$ and $R_{mr}$ one would compute the quality of the split as follows:\n$$\nQ_{s,p}^m = G(R_m) - [G(R_{ml}) + G(R_{mr})]\n$$\nThe splitting procedure computes the quality of all possible splits on all available predictors and selects the one with the highest quality after which it proceeds to further refine the newly formed sub-regions. For trees with scalar outputs, that approximate the true function with the local mean, the most appropriate cost function is the sum of squared residuals $G_{sse}(R_m)=\\sum_{y_i\\in R_m}(y_i-\\mu_{m})^2$. \n\nRegression trees have very high interpretative capabilities. Every step of the recursive partitioning procedure can be recorded in a form of a decision tree that visually puts the whole splitting process into perspective. An example of a recursive regression tree produced on an input space spanned by two parameters ($X_1$ and $X_2$) is given in figure below.\n\n![An example of a regression tree. Left: 2D input space. Right: The corresponding regression tree](/Users/ogyg/Google Drive/Functional_Research/PhD_Dissertation/Functional_Interpolation_Multivariate/img/Tree_example.png)\n\nRecursive splitting can be performed on pretty much any type of input parameters. Splitting on continuous input parameters is trivial, the data is simply ordered and the split point is moved from the lowest to the highest point of the parameters range. Categorical predictors have two cases, orderable and unorderable. For orderable categorical predictors the splitting procedure is the same as for continuous, while for unorderable one has to consider different combinations of categories. Obviously this becomes tedious and numerically difficult for a large number of categories. In the current version of `fTree` we allow for orderable predictors only. The implementation of the routines for splitting on categorical parameters is left for one of the future versions. However, at the end of this document we show a procedure for converting high dimensional predictors into orderable sequence that can be used directly with the current version of the `fTree` package. \n\nIn the `fTree` package we fit regression trees with the `ftree` function that takes the following inputs:\n\n- We pass a data frame of continuous predictors via `.X` input variable. The number of rows equals the number of observations while the number of columns equals the number of predictors.\n- The output data are provided via `.Y` variable. The code expects a data frame with functions or vectors stacked in columns. It is required that `nrow(.X) == ncol(.Y)`.\n- `.D` - specifies a user provided distance matrix (See below for more details).\n- `.SIGMA_inv` - specifies a user provided covariance matrix. This parameter is needed only for `mahalanobis` cost function. See below for more details.\n- `cost.type` - This parameter specifies the cost function to use in the tree fitting procedure. We explain all available cost functions in great detail below.\n- `tree.type` - This parameter specifies the desired tree type. Available options are: `single` that specifies that a single tree should be fitted, `bagging` that specifies that a bootstrapped tree should be fitted, and finally `randomforest` that specifies that a random forest should be fitted. \n- `nBoot` specified the number of bootstrapped samples to draw if `tree.type=\"bagging\"` or `tree.type=\"randomforest\"` is used\n- `nP` specifies the number of predictors to randomly consider in the `randomforest` fitting paradigm.\n- `.minSplit` is the minimum required number of observations that should be contained within a node before splitting is attempted. \n- `.minBucket` is the minimum number of elements in leaf nodes. \n- `.cp` - A split is accepted if its improvement in quality is at least `.cp * goodness` where goodness is the value of the cost function computed on the entire dataset (i.e. before splitting).\n- `ArgStep` - Argument step for functional data when the cost function is `sse`.\n\nWhat follows is a detailed discussion on the implemented cost functions and practical aspects of working with each one of them. In all our examples we will use a shale dataset with 13 predictors and 178 oil and gas production profiles (see the figure below). For more details about the dataset please consult chapter 4 of [Grujic (2017)](https://pangea.stanford.edu/ERE/pdf/pereports/PhD/Grujic2017.pdf?). \n\n```{r Load,  echo=FALSE, warning=FALSE, message=FALSE, comment=FALSE}\nlibrary(knitr)\nlibrary(fTree)\nlibrary(ggplot2)\nlibrary(functInterp)\nlibrary(plyr)\nload(\"~/Google Drive/Functional_Research/PhD_Dissertation/Functional_Interpolation_Multivariate/forVignette.RData\")\n```\n\n```{r Plot, fig.width=8, fig.height=4}\npar(mfrow=c(1,2))\nmatplot(OIL, type=\"l\", col=\"blue\", lty=1, xlab=\"Time (days)\", ylab=\"STB/day\", main=\"Oil Rates\")\nmatplot(GAS, type=\"l\", col=\"red\", lty=1, xlab=\"Time (days)\", ylab=\"MMCF/day\", main=\"Gas Rates\")\ntrain = 1:100\n```\n\n```{r Covariates, echo=FALSE}\nsummaryTable <- Reduce(cbind, apply(WELL_PARAMETERS, 2, function(x) functInterp::StatCompute(x, c(\"min\",\"sd\",\"mean\",\"median\",\"max\"))))\ncolnames(summaryTable) <- colnames(WELL_PARAMETERS)\nkable(t(summaryTable))\n```\n\n## Functional and Multivariate Regression Trees\n\n### Squared L2 Norm as a Cost Function\n\nWhen the outputs are vectors $\\boldsymbol{y}\\in R^n$, the simplest cost function can be formulated as follows:\n\n$$G(m) = \\sum_{i=1}^{N_m}\\|\\boldsymbol{y} - \\boldsymbol{\\mu_m}\\|^2$$\nWhere:\n\n- $m$ is the $m$-th node/region\n- $\\mu_m$ is the mean of the outputs contained in node $m$. \n\nIn the `ftree` function we refer to this cost function as `l2square` to indicate that it represents a squared L2 norm. The following code fits a regression tree with the `l2square` cost function:\n\n```{r L2squared, eval=FALSE}\nfTree.l2squared <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], cost.type = 'l2square', tree.type = 'single')\n```\n\n### L2 Norm as a Cost Function\n\nSimilarly we can also use a plain L2 norm as a cost function\n\n$$G(m) = \\sum_{i=1}^{N_m}\\|\\boldsymbol{y} - \\boldsymbol{\\mu_m}\\|$$\n\nIn the `ftree` function we refer to this cost function as `l2norm`. Note that in computer code implementation this cost function requires the computation of square roots and as such it is much slower than the previously considered squared l2 norm. The produced regression trees are always different though. The following code fits a tree with the `l2norm` cost function\n\n```{r L2norm, eval=FALSE}\nfTree.l2norm <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], cost.type = 'l2norm', tree.type = 'single')\n```\n\n### Mahalanobis Cost Function\nAnother type of a cost function for multivariate outputs was proposed by [Segal(1992)](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1992.10475220). The cost function computes the sum of Mahalanobis distances between the data and the regions mean vector\n\n$$G(m)=\\sum_{i=1}^{Nm}(\\boldsymbol{y}-\\boldsymbol{\\mu_m})^t\\Sigma_m^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu_m})$$\nIn the `ftree` function we refer to this cost function as `mahalanobis`. In the current version of the code, the `mahalanobis` cost function is the most computationally expensive out of all considered cost functions. In addition, the user needs to ensure that the empirical covariance matrix $\\Sigma_m$ is positive definite. When the covariance matrix is estimated from the training data, postive-definiteness is not guaranteed and in the case of indefiniteness, approximations need to be used. In the `ftree` function, we use the `nearPD` function from the `Matrix` package to approximate indefinite covariance matrices. The `nearPD` function computes the nearest positive definite covariance matrix by discarding eigen vectors associated with negative eigen values. We also allow for user provided inverse of the covariance matrix which can be passed through the `.SIGMA_inv` variable.\n\n```{r L2mahalanobis, eval=FALSE}\n# it computes the covariance matrix internally with nearPD.\nfTree.mahalanobis <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], cost.type = 'mahalanobis', tree.type = 'single')\n\n# or you can compute it externally and pass it in via .SIGMA_inv\nSIGMA_inv = as.matrix(solve(nearPD(cov(t(OIL[,train])))$mat))\nfTree.mahalanobis <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], .SIGMA_inv = SIGMA_inv, cost.type = 'mahalanobis', tree.type = 'single')\n\n```\n\n### SSE Cost Function\n\nWhen the output data are curves $y(t), t \\in T$, one can proceed in several ways. The simplest approach is to treat functions as vectors and employ one of the previously introduced cost functions. The dimensionality of the problem can be somewhat reduced by projecting the functional data onto the functional principal components and working with the fpc scores and the `l2square` or `l2norm` cost functions. Another approach is to use the sum of the integrated squared residuals as the cost function:\n\n$$G(m)=\\sum_{i=1}^{N_m}\\int_{T}(y(t)-\\mu(t))dt$$\n\nIn the `ftree` function, this cost function is computed with numerical (trapezoidal) integration with a user provided time step `ArgStep`. We refer to this cost function as `sse`. Note that the `sse` cost function produces very similar results as the `l2square` cost function when applied on the same data. \n\n```{r sse, eval=TRUE, warning=F, message=F, comment=F}\nfTree.sse <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], cost.type = 'sse', tree.type = 'single')\n```\n\n### A Similarity Distance-Based Cost Function\n\nAn alternative and numerically efficient way of growing regression trees with complex output data is by means of similarity distances. Starting from a matrix of similarity distances $D_m$ computed between the responses of some region (node) $m$ we can compute the following distance based cost function:\n\n$$G(m) = \\sum_{i=1}^{Nm}max(d_{ij})=\\sum_{i=1}^{Nm}d_{i,medoid}$$\nwhere $d_{i,medoid}$ is the distance of the $i$-th output from the most central output of the region $m$, the medoid. In the `ftree` function we refer to this cost function as `rdist`.\nThe `rdist` cost function is the most computationally efficient out of all other cost functions outlined in this document. Distances are computed only once before running the code and then the algorithm simply subsets the distance matrix at each iteration. This approach is also very robust since one can fit or test many different regression trees simply by changing the type of the similarity distance. Another convenience of this cost function is that it allows for computation with compound similarity distance matrices. For example, to fit a tree on multivariate functional data one would compute one distance matrix for each level of functional data, then scale the matrices to [0,1], and sum them up to produce a compound distance matrix. The compound distance matrix is then used with the `rdist` cost function to fit a regression tree.\n\nThere are two ways to grow regression trees with distances in `fTree`. The first approach is to compute a similarity distance matrix between all responses and pass it to the `ftree` function via `.D` parameter. Another approach is to pass the raw functional or vector data via `.Y` variable and set `.D` to the desired distance type, for example `.D='euclidean'`. The data contained in `.Y` and the distance type are passed to the generic `dist` function prior to growing a regression tree. \n\n```{r rdistO, eval=TRUE}\n# it computes the similarity distance matrix internally with `dist`.\nfTree.rdistO <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], .D = \"euclidean\", cost.type = 'rdist', tree.type = 'single')\n\n# Or you can compute your similiarity distance matrix externally and pass it in via .D parameter\nD = as.matrix(dist(t(OIL[,train])), \"euclidean\")\nfTree.rdistO <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], .D = D, cost.type = 'rdist', tree.type = 'single')\n```\n\nIf one wants to grow regression trees with multiple outputs then one can simply combine the distance matrices computed on each output type. Here we will demonstrate that approach on the oil and gas curves we introduced previously. \n\n```{r rdistOG, eval=TRUE}\nD.oil = as.matrix(dist(t(OIL[,train])), \"euclidean\")\nD.gas = as.matrix(dist(t(GAS[,train])), \"euclidean\")\nD.oil = D.oil/max(D.oil)\nD.gas = D.gas/max(D.gas)\nfTree.rdistOG <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], .D = D.oil + D.gas, cost.type = 'rdist', tree.type = 'single')\n```\n\nNote that in this mode we are still passing the oil rates to the `ftree` function via the `.Y` variable. The oil rates are not used for growing the tree in the `rdist` mode, they are only present for predictive purposes (more on that later).\n\nThe `ftree` function returns a list of the following elements:\n\n```{r ftreeOutput}\nstr(fTree.sse, 1)\n```\n\nThe `trees` slot contains the fitted tree saved in a form of a nested list. In this particular example the length of the `trees` slot is one since the fitted `tree.type` was `single`. In the case of bootstrapped trees the length of the `trees` object equals the value of the `nBoot` parameter. Each element of the nested list represents one node with the following elements:\n\n```{r ftreeOutputNode}\nstr(fTree.sse$trees[[1]], 1)\n```\n\nWhere:\n\n- `indices` is a vector of indices of the data contained within the node. \n- `nodeGoodness` represents the value of the cost function computed on the outputs of the current node.\n- `isLeaf` is a boolean parameter specifying wheter the node is a leaf node. This is used in prediction mode.\n- `nLeaves` represents the number of child leaves of the current node\n- `depth` represents the depth of the node in the recursive partitioning hierarchy\n- `midpoint` - a visualization parameter\n- `bestPredictor` represents the order of the predictor that splits the current node\n- `splitsGoodness` is a matrix of the best splits and their associated qualities of each predictor for the current node. Note that `midpoint` and `bestPredictor` are obtained from this matrix. The matrix is used in variable importance computations. \n- `splitPoint` represents the split point of the `bestPredictor` for splitting the current node \n- `left` is a sublist containing the data and the nodes that are on the \"left\" after splitting on the `bestPredictor` at `splitPoint`\n- `right` is a right sublist containing the data and the nodes that remain on the right after splitting the current node at `bestPredictor` at `splitPoint`\n\n## How To Select a Cost Function?\n\nThe selection of the cost function is problem dependent. In some settings a simple cost function such as `l2square` would do a good job, while in some other cases a complex distance computation (i.e. Modified Hausdorff) has to be performed along with the `rdist` cost function. The best way is to try different options and assess the results in terms of whether or not the modeling objectives are being met. To ease the selection process we expose two convenience functions that enable easier data mining of the fitted regression trees. The first function takes a node flag as an input and it returns the indices of the data contained in the specified node. The returned indices are relative to the original ordering of the data. The user can then plot the data with custom coded routines. For example, one might consider plotting vectors in low dimensional multidimensional scaled plots (i.e. `cmdscale`) or simply use `matplot` on the subsetted functional data. \n\nThe second function is using the same node flagging as the previous one, however its output is a plot that marks the selected node in the tree topography. The combination of these two functions enables easy data mining of the fitted regression tree and an easy visual assessment of the utilized cost function(s).\n\n```{r Plotting, fig.width=4, fig.height=3.5, warning=F, comment=F, message=F, fig.show=\"hold\"}\nnode = \"0ll\"\nplotFtree(fTree.sse, .labSize = 2, .horizontal = T, .ylimLow = -1.5, .node = node, .round = 2)\nindices <- getNodeIndices(fTree.sse$trees[[1]], node)\nmatplot(OIL[,train], type=\"l\", col=\"black\", lty=1, xlab=\"Time (days)\", ylab=\"STB/day\")\nmatplot(OIL[,train][,indices$rightInd], type = \"l\", col=\"blue\", lty=1, add=TRUE)\nmatplot(OIL[,train][,indices$leftInd], type = \"l\", col=\"red\", lty=1, add=TRUE)\n```\n\n# Bootstrapped Regression Trees\n\nThere are two modeling paradigms when it comes to bootstrapped regression trees. The first modeling paradigm considers simple bootstrapping of the training set and it fits one regression tree to each bootstrapped sample. This paradigm is referred to as \"bagging\" both in the literature and in the `ftree` function (i.e. `tree.type='bagging'`). The second bootstrapping paradigm bootstraps the training data twice. First it bootstraps the observations then on each split in the tree fitting procedure it selects only a randomly selected subset of all available predictors. This bootstrapping paradigm is commonly referred to as \"random forest\" both in the code and in the literature (i.e. `tree.type='randomforest'`). Bootstrapped regression trees can be fitted with all of the aforementioned cost functions simply by changing the `tree.type` parameter. For example:\n\n```{r Boostrap, eval=TRUE}\nfTree.rdistO <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], .D = D.oil, cost.type = 'rdist', tree.type = 'randomforest', nBoot = 200)\nfTree.rdistOG <- ftree(.X = WELL_PARAMETERS[train,], .Y=OIL[,train], .D = D.oil + D.gas, cost.type = 'rdist', tree.type = 'randomforest')\n```\n\n# Variable Importance\n\nIn the current version of the code, variable importances can be computed only on `single` and `bagging` tree types. In one of the future versions of the code we plan on implementing the permutation scheme for random forests. In the current version of the code surrogate splitting is not performed and on each node we save the quality of the best split produced by each predictor. Variable importance computations are simply averaging the qualities of the best split of each predictor over all nodes. In other words:\n\n$$\nS_p = \\frac{1}{M}\\sum_{m=1}^{M}\\frac{1}{N_m}max\\{Q_{s,p}^{m}, \\forall s \\}\n$$\nWhere: \n\n- $Q_{s,p}^m$ is the quality of the best split produced by splitting the node $m$ by parameter $p$, \n- $M$ is the number of splits in a tree and \n- $N_m$ is the number of training points within the node $m$.\n\nThe following code computes variable importance plots for the two distance based trees we fitted earlier. \n\n```{r VarImpCode, eval = FALSE}\nvarSensitivity(fTree.rdistO)\nvarsensitivity(fTree.rdistOG)\n```\n\n```{r VarImpPlot, eval = TRUE, echo=FALSE, fig.height=5, fig.width=8}\np1 <- varSensitivity(fTree.rdistO, mode = \"ggRet\") + ggtitle(\"Oil Only\")\np2 <- varSensitivity(fTree.rdistOG, mode = \"ggRet\") + ggtitle(\"Oil and Gas\")\nmultiPlot(p1,p2, cols=2)\n```\n\n# Making Predictions With `fTree`\n\nTo make predictions with the `fTree` package we use the `predictFtree` function. The function takes two inputs. The first input is a fitted `fTree` structure, while the second input is a data frame `.Xnew` that contains the input parameters of the new data. The code returns a list of length `nrow(.Xnew)`. In the case of bootstrapped trees, each element of the list is a data frame whose number of rows equals the `nBoot` parameter we used when building the tree (with `ftree`).  \n\n```{r predictOil}\nfTree.rdistO.predict <- predictFtree(fTree.rdistO, .Xnew = WELL_PARAMETERS[-train,])\n```\n\nIn the case of multivariate similarity distance based tree, in the current version of `fTree` we cannot forecast both outputs at the same time. Instead we need to employ a small workaround solution demonstrated in the following code.\n\n```{r predictOilGas}\nfTree.rdistOG.predictO <- predictFtree(fTree.rdistOG, .Xnew = WELL_PARAMETERS[-train,])\nfTree.rdistOG.Gas <- fTree.rdistOG\nfTree.rdistOG.Gas$functions <- GAS[,train]\nfTree.rdistOG.predictG <- predictFtree(fTree.rdistOG.Gas, .Xnew = WELL_PARAMETERS[-train,])\n```\n\nWe can plot the results with the following code:\n```{r PlotPredictions, fig.width=7, fig.height=7, fig.cap=\"A few forecasts of oil and gas rates. Each row is one well, left column are oil rates while the right column are gas rates. Colored lines are the predictions, black lines are the true responses\"}\npar(mfrow=c(4,2))\npar(mar=c(4,4,1,1)+0.1)\nfor(i in 1:4){\nmatplot(t(fTree.rdistOG.predictO[[i]]),type=\"l\", col=\"blue\", lty=1, xlab=\"Time(days)\", ylab=\"STB/day\", ylim=c(0,700))\nlines(OIL[,-train][,i], col=\"black\", lty=1, lwd=2) \n\nmatplot(t(fTree.rdistOG.predictG[[i]]), col=\"red\", lty=1, xlab=\"Time(days)\", ylab=\"MSCF/day\", type=\"l\", ylim=c(0,1600))\nlines(GAS[,-train][,i], col=\"black\", lty=1, lwd=2) \n\n}\n```\n# Functional Predictors\n\nThe package does not implement the routines for growing regression trees with functional predictors, however the data can be pre-processed such that it is presented to the `ftree` function as a continuous predictor. The procedure we propose for such data transformation is as follows. \n\n1. Compute similarity distances between the observations of a complex predictor (i.e. with the `dist` function)\n2. Cluster the functional or complex predictor data with hierarchical clustering (i.e. with the `hclust` function)\n3. Use Bar Joseph reordering algorithm to re-order the leafs of the hierarchical clustering tree.\n4. Present the new ordering of the complex predictor as a continuous predictor to the `fTree` function.\n\nThe following is a visual representation of the procedure.\n\n![An example of ordering of complex predictors. A - Raw unordered functional predictor data. B - Low dimensional (MDS) representation of the data colored by original (old) ordering. C - Hierarchical clustering performed on the raw data. D - Leaf reordered hierarchical clustering dendrogram. E - A low dimensional representation (MDS) colored by the new ordering of the data. F - A plot of the original data colored by the new ordering. (from [Grujic (2017)](https://pangea.stanford.edu/ERE/pdf/pereports/PhD/Grujic2017.pdf?))](/Users/ogyg/Google Drive/Functional_Research/PhD_Dissertation/Functional_Interpolation_Multivariate/img/functional_predictor.png)\n\n\n\n# Writing Custom Cost Functions (Under Development)\n\n\n\n\n",
    "created" : 1510289005248.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2723793714",
    "id" : "445004E2",
    "lastKnownWriteTime" : 1510967857,
    "last_content_update" : 1510967857745,
    "path" : "~/Dropbox/TreesPaper/package/fTree/vignette/first_steps.Rmd",
    "project_path" : "vignette/first_steps.Rmd",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}